<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Joey&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Welcome to my blog!">
<meta property="og:type" content="website">
<meta property="og:title" content="Joey&#39;s Blog">
<meta property="og:url" content="https://ichika2004.github.io/Blog/index.html">
<meta property="og:site_name" content="Joey&#39;s Blog">
<meta property="og:description" content="Welcome to my blog!">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Joey">
<meta property="article:tag" content="Blog">
<meta property="article:tag" content="Joey">
<meta property="article:tag" content="Personal">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/Blog/atom.xml" title="Joey's Blog" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/Blog/favicon.png">
  
  
  
<link rel="stylesheet" href="/Blog/css/style.css">

  
    
<link rel="stylesheet" href="/Blog/fancybox/jquery.fancybox.min.css">

  
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.2.0/css/fork-awesome.min.css">

<meta name="generator" content="Hexo 8.1.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/Blog/" id="logo">Joey&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/Blog/" id="subtitle">A personal blog by Joey</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/Blog/">Home</a>
        
          <a class="main-nav-link" href="/Blog/archives">Archives</a>
        
          <a class="main-nav-link" href="/Blog/categories/bioinformatics/">生物資訊初探</a>
        
          <a class="main-nav-link" href="/Blog/categories/machine-learning/">機器學習筆記</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/Blog/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://ichika2004.github.io/Blog"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-第一週" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/Blog/2026/02/24/%E7%AC%AC%E4%B8%80%E9%80%B1/" class="article-date">
  <time class="dt-published" datetime="2026-02-24T06:00:00.000Z" itemprop="datePublished">2026-02-24</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/Blog/categories/bioinformatics/">生物資訊初探</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/Blog/2026/02/24/%E7%AC%AC%E4%B8%80%E9%80%B1/">Week 1</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <H3>前言-課程介紹：</H3>
* 評分標準
1. 課堂問答+Kahoot 25%
2. 課堂筆記 25% (寫在部落格上，期末需要有8篇)
3. 期中上機測驗25%
4. 期末小組(最多2人一組)報告 （25%）

<ul>
<li>課程進行方式</li>
</ul>
<ol>
<li>看課前閱讀資料</li>
<li>課堂主題說明</li>
<li>有課堂問答 跟 Kahoot 每節課會競賽排名</li>
<li>動手操作，將生物的問題轉化為工程或數學問題</li>
<li>看看已經發展出的生物資訊工具如何解決這類問題</li>
<li>更新自己的部落格，並參加 Kahoot 每節課會競賽排名</li>
<li>談談這種工具在生活中的實際應用</li>
</ol>
<ul>
<li>課程目標</li>
</ul>
<ol>
<li>介紹生物資訊工具的應用</li>
<li>以段落與專題的方式介紹常用的基礎生物資訊工具與資料庫應用</li>
<li>知道如何分析運用生物資訊</li>
</ol>
<H3>【生物資訊學概論：數據如何定義現代生物學？】</H3>
    
<p>隨著測序技術的普及，生物學研究已從單純的實驗室觀察，演進為大數據分析的科學。</p>
<p>一、 生物資訊學的定義與範疇生物資訊學是一門結合生物學、電腦科學與統計學的跨領域學科。其核心任務包含：</p>
<ul>
<li>數據管理：開發資料庫以儲存海量的生物分子資訊（如 DNA、RNA、蛋白質序列）。</li>
<li>演算法開發：設計統計模型與運算工具，分析序列間的相似性與演化關係。</li>
<li>知識發現：透過數據挖掘，解釋生物系統的功能與機制。</li>
</ul>
<p>二、 為什麼需要生物資訊？</p>
<p>實務案例分析生物資訊並非僅限於理論研究，在實際生活與法律鑑定中扮演重要角色。例如：</p>
<p>物種鑑定（Species Identification）：透過 DNA 序列比對（DNA Barcoding），可以精準鑑定食品成分。例如，過去曾有調查局利用 DNA 分析技術，揭發市售素食中摻雜豬、牛肉成分，或肉製品中混有非標示物種。這類鑑定需依賴精確的生物序列資料庫與比對演算法。</p>
<p>三、 生物學的演進歷程生物資訊的興起與分子生物學的進程緊密相連：</p>
<p>1860年代：孟德爾提出遺傳定律，建立遺傳學雛形。<br>1953年：DNA 雙螺旋結構被發現，開啟分子生物學時代。<br>1977年：Sanger 定序法問世，使人類獲得讀取遺傳密碼的能力。<br>1990-2003年：人類基因組計畫（Human Genome Project）的執行，標誌著基因組學（Genomics）的誕生，也使生物資訊學成為處理這些龐大數據的必要工具。</p>
<p>四、 核心工具介紹：NCBI </p>
<p>美國國家生物技術資訊中心（NCBI）是全球最重要的生物數據中心之一。</p>
<p>五、 Week1總結與展望</p>
<p>現代生物學正處於「第四範式」（數據驅動）的階段。掌握生物資訊工具，不僅是為了研究，更是為了能從海量的遺傳資訊中，精確地解析生命現象。對於初學者而言，熟悉 NCBI 等公開平台的檢索與應用，是進入此領域的第一步。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://ichika2004.github.io/Blog/2026/02/24/%E7%AC%AC%E4%B8%80%E9%80%B1/" data-id="cuidIJFJk3mycJ0JWVeJJu4g-" data-title="Week 1" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Diabetes-prediction-using-K-Nearest-Neighbors" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/Blog/2023/05/19/Diabetes-prediction-using-K-Nearest-Neighbors/" class="article-date">
  <time class="dt-published" datetime="2023-05-19T02:56:06.000Z" itemprop="datePublished">2023-05-19</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/Blog/categories/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92/">機器學習</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/Blog/2023/05/19/Diabetes-prediction-using-K-Nearest-Neighbors/">機器學習入門：k-NN (k-Nearest Neighbors) 演算法詳解</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="機器學習入門：k-NN-k-Nearest-Neighbors-演算法詳解"><a href="#機器學習入門：k-NN-k-Nearest-Neighbors-演算法詳解" class="headerlink" title="機器學習入門：k-NN (k-Nearest Neighbors) 演算法詳解"></a>機器學習入門：k-NN (k-Nearest Neighbors) 演算法詳解</h1><p>在機器學習的世界裡，<strong>k-NN (k-最近鄰)</strong> 演算法以其簡單、直觀的特性，成為許多初學者的第一個入門演算法。它不需要複雜的數學推導，核心思想只有一句話：「物以類聚」。</p>
<h2 id="1-什麼是-k-NN-演算法？"><a href="#1-什麼是-k-NN-演算法？" class="headerlink" title="1. 什麼是 k-NN 演算法？"></a>1. 什麼是 k-NN 演算法？</h2><p>k-NN 是一種<strong>監督式學習 (Supervised Learning)</strong> 演算法，既可以用於<strong>分類 (Classification)</strong>，也可以用於<strong>回歸 (Regression)</strong>。</p>
<p>它的工作原理非常直觀：當我們要預測一個新數據點的類別時，演算法會在訓練資料集中尋找與該點「距離最近」的 <strong>k</strong> 個鄰居。接著，根據這 k 個鄰居的類別進行「投票」，票數最多的類別就是該新數據點的預測結果。</p>
<hr>
<h2 id="2-k-NN-的運作步驟"><a href="#2-k-NN-的運作步驟" class="headerlink" title="2. k-NN 的運作步驟"></a>2. k-NN 的運作步驟</h2><ol>
<li><strong>選擇 k 值</strong>：決定要參考多少個鄰居（例如 k&#x3D;3 或 k&#x3D;5）。</li>
<li><strong>計算距離</strong>：計算新數據點與訓練集中所有點的距離。</li>
<li><strong>尋找鄰居</strong>：找出距離最近的 k 個點。</li>
<li><strong>進行預測</strong>：<ul>
<li><strong>分類問題</strong>：採用「多數決」，將新點歸類為鄰居中出現次數最多的類別。</li>
<li><strong>回歸問題</strong>：計算 k 個鄰居數值的「平均值」作為預測結果。</li>
</ul>
</li>
</ol>

<hr>
<h2 id="3-如何計算「距離」？"><a href="#3-如何計算「距離」？" class="headerlink" title="3. 如何計算「距離」？"></a>3. 如何計算「距離」？</h2><p>「距離」的定義決定了鄰居的選取。最常用的計算方法包括：</p>
<ul>
<li><strong>歐氏距離 (Euclidean Distance)</strong>：最常見的直線距離（L2 範數）。<br>  $$d &#x3D; \sqrt{\sum_{i&#x3D;1}^{n} (x_i - y_i)^2}$$</li>
<li><strong>曼哈頓距離 (Manhattan Distance)</strong>：城市街區距離（L1 範數）。</li>
<li><strong>閔可夫斯基距離 (Minkowski Distance)</strong>：上述兩者的推廣形式。</li>
</ul>
<blockquote>
<p><strong>注意</strong>：由於距離計算對數值大小敏感，在使用 k-NN 前，務必對數據進行 <strong>特徵縮放 (Feature Scaling)</strong>（如標準化或歸一化），否則數值較大的特徵會主導距離運算。</p>
</blockquote>
<hr>
<h2 id="4-如何選擇關鍵的-k-值？"><a href="#4-如何選擇關鍵的-k-值？" class="headerlink" title="4. 如何選擇關鍵的 k 值？"></a>4. 如何選擇關鍵的 k 值？</h2><p>k 值的選擇會大幅影響模型的表現：</p>
<ul>
<li><strong>k 太小（如 k&#x3D;1）</strong>：模型會對噪聲非常敏感，容易產生<strong>過擬合 (Overfitting)</strong>。</li>
<li><strong>k 太大（如 k&#x3D;100）</strong>：邊界會變得模糊，容易產生<strong>欠擬合 (Underfitting)</strong>。</li>
</ul>
<p><strong>最佳實作建議</strong>：通常使用<strong>交叉驗證 (Cross-validation)</strong> 來尋找最適合該資料集的 k 值。此外，為了避免平手（票數相同），k 通常建議選擇<strong>奇數</strong>。</p>
<hr>
<h2 id="5-k-NN-的優缺點分析"><a href="#5-k-NN-的優缺點分析" class="headerlink" title="5. k-NN 的優缺點分析"></a>5. k-NN 的優缺點分析</h2><h3 id="優點"><a href="#優點" class="headerlink" title="優點"></a>優點</h3><ul>
<li><strong>簡單易懂</strong>：不需建立複雜模型，易於實現。</li>
<li><strong>無假設</strong>：屬於非參數化模型，不假設數據分佈。</li>
<li><strong>適合多分類</strong>：處理多種類別的分類問題非常直觀。</li>
</ul>
<h3 id="缺點"><a href="#缺點" class="headerlink" title="缺點"></a>缺點</h3><ul>
<li><strong>計算代價高</strong>：每次預測都要掃描整個資料集計算距離，當數據量大時速度極慢。</li>
<li><strong>記憶體消耗大</strong>：需要儲存所有的訓練數據。</li>
<li><strong>維度災難</strong>：在高維度空間中，點與點之間的距離會變得很近，導致分類效果變差。</li>
</ul>
<p>k-NN 雖然在處理大規模數據時效率不高，但其「懶惰學習 (Lazy Learning)」的特性（不需要事先訓練模型），使其在小型數據集或異常檢測任務中仍非常實用。</p>
<hr>
<H3>用 Python 從零實作 k-NN 演算法與進階加權技巧</H3>

<p>在學習機器學習時，直接套用 <code>scikit-learn</code> 固然快速，但若能親手用原生 Python 刻出演算法的底層邏輯，對觀念的理解將會大躍進。今天我們將以<strong>糖尿病預測資料集 (Diabetes Dataset)</strong> 為例，不依賴機器學習套件，從零開始實作 <strong>k-NN (k-Nearest Neighbors) 演算法</strong>，並進階探討「曼哈頓距離」與「距離加權」的優化技巧。</p>
<p>資料集來源：<a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/johndasilva/diabetes">https://www.kaggle.com/datasets/johndasilva/diabetes</a></p>
<p>訓練集樣本數：567</p>
<p>Code:<br><a target="_blank" rel="noopener" href="https://github.com/ichika2004/kNN/blob/main/Data_mining.ipynb">https://github.com/ichika2004/kNN/blob/main/Data_mining.ipynb</a></p>
<hr>
<h2 id="1-資料預處理：為什麼特徵縮放如此重要？"><a href="#1-資料預處理：為什麼特徵縮放如此重要？" class="headerlink" title="1. 資料預處理：為什麼特徵縮放如此重要？"></a>1. 資料預處理：為什麼特徵縮放如此重要？</h2><p>k-NN 是一個高度依賴「距離」的演算法。如果資料集中某個特徵的數值極大（例如胰島素濃度），而另一個特徵的數值極小（例如懷孕次數），在計算距離時，大數值的特徵就會完全主導結果。</p>
<p>因此，在進入模型之前，我們必須自己寫一個標準化（Z-score Standardization）的函數，將所有特徵縮放到相同的尺度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 標準化函數：(數值 - 平均值) / 標準差</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">standardize</span>(<span class="params">df</span>):</span><br><span class="line">    df = (df - df.mean()) / df.std()</span><br><span class="line">    <span class="keyword">return</span> df</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="2-核心實作：基礎-k-NN-歐氏距離與多數決"><a href="#2-核心實作：基礎-k-NN-歐氏距離與多數決" class="headerlink" title="2. 核心實作：基礎 k-NN (歐氏距離與多數決)"></a>2. 核心實作：基礎 k-NN (歐氏距離與多數決)</h2><p>定義 <code>knn</code> 函數：針對測試集中的每一筆資料，我們需要：</p>
<ol>
<li>計算它與訓練集中所有資料的<strong>歐氏距離 (Euclidean Distance)</strong>。</li>
<li>將距離由小到大排序。</li>
<li>取出距離最近的 $K$ 個鄰居。</li>
<li>進行「多數決」投票，票數高者即為預測結果。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">knn</span>(<span class="params">train_data, test_data, k</span>):</span><br><span class="line">    predictions = []</span><br><span class="line">    <span class="keyword">for</span> index, test_instance <span class="keyword">in</span> test_data.iterrows():</span><br><span class="line">        distances = []</span><br><span class="line">        <span class="comment"># 1. 計算與所有訓練資料的距離</span></span><br><span class="line">        <span class="keyword">for</span> _, train_instance <span class="keyword">in</span> train_data.iterrows():</span><br><span class="line">            dist = mt.sqrt((test_instance[<span class="number">0</span>] - train_instance[<span class="number">0</span>])**<span class="number">2</span> +</span><br><span class="line">                           (test_instance[<span class="number">1</span>] - train_instance[<span class="number">1</span>])**<span class="number">2</span> +</span><br><span class="line">                           (test_instance[<span class="number">2</span>] - train_instance[<span class="number">2</span>])**<span class="number">2</span> +</span><br><span class="line">                           (test_instance[<span class="number">3</span>] - train_instance[<span class="number">3</span>])**<span class="number">2</span> +</span><br><span class="line">                           (test_instance[<span class="number">4</span>] - train_instance[<span class="number">4</span>])**<span class="number">2</span> +</span><br><span class="line">                           (test_instance[<span class="number">5</span>] - train_instance[<span class="number">5</span>])**<span class="number">2</span> +</span><br><span class="line">                           (test_instance[<span class="number">6</span>] - train_instance[<span class="number">6</span>])**<span class="number">2</span> +</span><br><span class="line">                           (test_instance[<span class="number">7</span>] - train_instance[<span class="number">7</span>])**<span class="number">2</span>)</span><br><span class="line">            distances.append((dist, train_instance[<span class="string">&#x27;Outcome&#x27;</span>]))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. 距離由小到大排序</span></span><br><span class="line">        distances.sort(key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>])</span><br><span class="line">        <span class="comment"># 3. 尋找最近的 k 個鄰居</span></span><br><span class="line">        neighbors = distances[:k] </span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4. 多數決投票</span></span><br><span class="line">        votes = &#123;<span class="string">&#x27;0&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;1&#x27;</span>: <span class="number">0</span>&#125;</span><br><span class="line">        <span class="keyword">for</span> _, outcome <span class="keyword">in</span> neighbors:</span><br><span class="line">            votes[<span class="built_in">str</span>(<span class="built_in">int</span>(outcome))] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        predicted_class = <span class="built_in">max</span>(votes, key=votes.get)</span><br><span class="line">        predictions.append(<span class="built_in">int</span>(predicted_class))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> predictions</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="3-尋找最佳的-K-值"><a href="#3-尋找最佳的-K-值" class="headerlink" title="3. 尋找最佳的 K 值"></a>3. 尋找最佳的 K 值</h2><p>K 值的選擇會大幅影響準確率。一般來說，K 值建議選擇<strong>奇數</strong>（避免平手），且最大搜尋範圍通常設定在<strong>訓練集資料筆數的平方根</strong>。我們可以使用迴圈來測試不同的 K 值，並視覺化結果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 取得訓練資料筆數的平方根做為最大 K 值</span></span><br><span class="line">k_max = <span class="built_in">int</span>(mt.sqrt(<span class="built_in">len</span>(train_data)))  </span><br><span class="line">k_values = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">1</span>, k_max, <span class="number">2</span>))</span><br><span class="line">accuracy_euil = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 測試不同的 K 值</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> k_values:</span><br><span class="line">    predictions = knn(train_data, test_data, k)</span><br><span class="line">    true_outcome = test_data[<span class="string">&#x27;Outcome&#x27;</span>].tolist()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 計算準確率</span></span><br><span class="line">    correct_predictions = <span class="built_in">sum</span>(<span class="number">1</span> <span class="keyword">for</span> true, pred <span class="keyword">in</span> <span class="built_in">zip</span>(true_outcome, predictions) <span class="keyword">if</span> true == pred)</span><br><span class="line">    accuracy = correct_predictions / <span class="built_in">len</span>(true_outcome)</span><br><span class="line">    accuracy_euil.append(accuracy)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<hr>
<h2 id="4-進階實作-一-：變更距離標準-曼哈頓距離"><a href="#4-進階實作-一-：變更距離標準-曼哈頓距離" class="headerlink" title="4. 進階實作 (一)：變更距離標準 - 曼哈頓距離"></a>4. 進階實作 (一)：變更距離標準 - 曼哈頓距離</h2><p>除了直線的歐氏距離，有時候在特定維度特徵下，使用<strong>曼哈頓距離 (Manhattan Distance, 絕對值相加)</strong> 會有更好的效果。實作上，我們只需要修改距離的計算方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 曼哈頓距離計算範例</span></span><br><span class="line">dist = (<span class="built_in">abs</span>(test_instance[<span class="number">0</span>] - train_instance[<span class="number">0</span>]) +</span><br><span class="line">        <span class="built_in">abs</span>(test_instance[<span class="number">1</span>] - train_instance[<span class="number">1</span>]) +</span><br><span class="line">        <span class="built_in">abs</span>(test_instance[<span class="number">2</span>] - train_instance[<span class="number">2</span>]) +</span><br><span class="line">        <span class="comment"># ...省略其餘特徵...</span></span><br><span class="line">        <span class="built_in">abs</span>(test_instance[<span class="number">7</span>] - train_instance[<span class="number">7</span>]))</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<hr>
<h2 id="5-進階實作-二-：距離加權投票-Weighted-k-NN"><a href="#5-進階實作-二-：距離加權投票-Weighted-k-NN" class="headerlink" title="5. 進階實作 (二)：距離加權投票 (Weighted k-NN)"></a>5. 進階實作 (二)：距離加權投票 (Weighted k-NN)</h2><p>傳統 k-NN 有個缺點：「只要擠進前 K 名，每一票的份量都一樣重」。但直覺上，<strong>距離越近的鄰居，應該要有越高的參考價值</strong>。這時我們可以引入「加權機制」。</p>
<h3 id="方法-1：反函數加權-Inverse-Distance-Weighting"><a href="#方法-1：反函數加權-Inverse-Distance-Weighting" class="headerlink" title="方法 1：反函數加權 (Inverse Distance Weighting)"></a>方法 1：反函數加權 (Inverse Distance Weighting)</h3><p>將距離取倒數，距離越近，權重越大。（加上 $10^{-10}$ 是為了避免距離為 0 時發生除以零的錯誤）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">votes = &#123;<span class="string">&#x27;0&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;1&#x27;</span>: <span class="number">0</span>&#125;</span><br><span class="line"><span class="keyword">for</span> dist, outcome <span class="keyword">in</span> neighbors:</span><br><span class="line">    votes[<span class="built_in">str</span>(<span class="built_in">int</span>(outcome))] += <span class="number">1</span> / (dist + <span class="number">10</span>**(-<span class="number">10</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h3 id="方法-2：高斯函數加權-Gaussian-Weighting"><a href="#方法-2：高斯函數加權-Gaussian-Weighting" class="headerlink" title="方法 2：高斯函數加權 (Gaussian Weighting)"></a>方法 2：高斯函數加權 (Gaussian Weighting)</h3><p>高斯函數（常態分佈曲線）是一種更平滑的加權方式。給定一個參數 <code>sigma</code> 來控制權重遞減的速度。</p>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sigma 為自訂參數</span></span><br><span class="line"><span class="keyword">for</span> _, train_instance <span class="keyword">in</span> train_data.iterrows():</span><br><span class="line">    <span class="comment"># ...前面計算歐氏距離 (dist)...</span></span><br><span class="line">    weight = mt.exp(-dist**<span class="number">2</span> / (<span class="number">2</span> * (sigma**<span class="number">2</span>)))</span><br><span class="line">    distances.append((weight, train_instance[<span class="string">&#x27;Outcome&#x27;</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 依照權重由大到小排序 (因為權重越大代表越近)</span></span><br><span class="line">distances.sort(key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">neighbors = distances[:k]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> weight, outcome <span class="keyword">in</span> neighbors:</span><br><span class="line">    votes[<span class="built_in">str</span>(<span class="built_in">int</span>(outcome))] += weight</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p><strong>討論：為何反函數加權結果與一般K-NN相似</strong></p>
<p>這點可以從高斯函數解釋，當執行標準化後，資料尺度被縮到(-1,1)，導致其權重相差不大</p>
<hr>
<h2 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h2><p>透過親手實作 <code>knn</code> 函數，我們不僅徹底理解了資料標準化的重要性，也掌握了如何抽換「距離公式」與「投票加權機制」。這些底層邏輯的熟悉度，將成為未來在微調機器學習模型參數時，最強大的基礎！</p>
<p><strong>標籤：</strong> #Python #機器學習 #DataMining #kNN演算法 #演算法實作 #資料科學</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://ichika2004.github.io/Blog/2023/05/19/Diabetes-prediction-using-K-Nearest-Neighbors/" data-id="cuidBz3i4eYfjPs1dk5m6OyOk" data-title="機器學習入門：k-NN (k-Nearest Neighbors) 演算法詳解" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/Blog/categories/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92/">機器學習</a></li><li class="category-list-item"><a class="category-list-link" href="/Blog/categories/bioinformatics/">生物資訊初探</a></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/Blog/archives/2026/02/">February 2026</a></li><li class="archive-list-item"><a class="archive-list-link" href="/Blog/archives/2023/05/">May 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/Blog/2026/02/24/%E7%AC%AC%E4%B8%80%E9%80%B1/">Week 1</a>
          </li>
        
          <li>
            <a href="/Blog/2023/05/19/Diabetes-prediction-using-K-Nearest-Neighbors/">機器學習入門：k-NN (k-Nearest Neighbors) 演算法詳解</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2026 Joey<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/Blog/" class="mobile-nav-link">Home</a>
  
    <a href="/Blog/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/Blog/categories/bioinformatics/" class="mobile-nav-link">生物資訊初探</a>
  
    <a href="/Blog/categories/machine-learning/" class="mobile-nav-link">機器學習筆記</a>
  
</nav>
    


<script src="/Blog/js/jquery-3.6.4.min.js"></script>



  
<script src="/Blog/fancybox/jquery.fancybox.min.js"></script>




<script src="/Blog/js/script.js"></script>





  </div>
</body>
</html>