<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>機器學習入門：k-NN (k-Nearest Neighbors) 演算法詳解 | Joey&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="在機器學習的世界裡，k-NN (k-最近鄰) 演算法以其簡單、直觀的特性，成為許多初學者的第一個入門演算法。它不需要複雜的數學推導，核心思想只有一句話：「物以類聚」。 1. 什麼是 k-NN 演算法？k-NN 是一種監督式學習 (Supervised Learning) 演算法，既可以用於分類 (Classification)，也可以用於回歸 (Regression)。 它的工作原理非常直觀：當我">
<meta property="og:type" content="article">
<meta property="og:title" content="機器學習入門：k-NN (k-Nearest Neighbors) 演算法詳解">
<meta property="og:url" content="https://ichika2004.github.io/Blog/2023/05/19/Diabetes-prediction-using-K-Nearest-Neighbors/index.html">
<meta property="og:site_name" content="Joey&#39;s Blog">
<meta property="og:description" content="在機器學習的世界裡，k-NN (k-最近鄰) 演算法以其簡單、直觀的特性，成為許多初學者的第一個入門演算法。它不需要複雜的數學推導，核心思想只有一句話：「物以類聚」。 1. 什麼是 k-NN 演算法？k-NN 是一種監督式學習 (Supervised Learning) 演算法，既可以用於分類 (Classification)，也可以用於回歸 (Regression)。 它的工作原理非常直觀：當我">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://hackmd.io/_uploads/BJPCNfnubg.png">
<meta property="og:image" content="https://hackmd.io/_uploads/B10tWbn_Ze.png">
<meta property="og:image" content="https://hackmd.io/_uploads/SJxfm-hOZx.png">
<meta property="og:image" content="https://hackmd.io/_uploads/BJmv7ZnuWx.png">
<meta property="og:image" content="https://hackmd.io/_uploads/B1iiX-2OZe.png">
<meta property="og:image" content="https://hackmd.io/_uploads/B1IR7b2Obg.png">
<meta property="article:published_time" content="2023-05-19T02:56:06.000Z">
<meta property="article:modified_time" content="2026-02-25T06:23:19.933Z">
<meta property="article:author" content="Joey">
<meta property="article:tag" content="Blog">
<meta property="article:tag" content="Joey">
<meta property="article:tag" content="Personal">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://hackmd.io/_uploads/BJPCNfnubg.png">
  
    <link rel="alternate" href="/Blog/atom.xml" title="Joey's Blog" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/Blog/favicon.png">
  
  
  
<link rel="stylesheet" href="/Blog/css/style.css">

  
    
<link rel="stylesheet" href="/Blog/fancybox/jquery.fancybox.min.css">

  
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.2.0/css/fork-awesome.min.css">

<meta name="generator" content="Hexo 8.1.1"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/Blog/" id="logo">Joey&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/Blog/" id="subtitle">A personal blog by Joey</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/Blog/">Home</a>
        
          <a class="main-nav-link" href="/Blog/archives">Archives</a>
        
          <a class="main-nav-link" href="/Blog/categories/bioinformatics/">生物資訊初探</a>
        
          <a class="main-nav-link" href="/Blog/categories/machine-learning/">機器學習筆記</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/Blog/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://ichika2004.github.io/Blog"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-Diabetes-prediction-using-K-Nearest-Neighbors" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/Blog/2023/05/19/Diabetes-prediction-using-K-Nearest-Neighbors/" class="article-date">
  <time class="dt-published" datetime="2023-05-19T02:56:06.000Z" itemprop="datePublished">2023-05-19</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/Blog/categories/machine-learning/">機器學習筆記</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      機器學習入門：k-NN (k-Nearest Neighbors) 演算法詳解
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>在機器學習的世界裡，<strong>k-NN (k-最近鄰)</strong> 演算法以其簡單、直觀的特性，成為許多初學者的第一個入門演算法。它不需要複雜的數學推導，核心思想只有一句話：「物以類聚」。</p>
<h2 id="1-什麼是-k-NN-演算法？"><a href="#1-什麼是-k-NN-演算法？" class="headerlink" title="1. 什麼是 k-NN 演算法？"></a>1. 什麼是 k-NN 演算法？</h2><p>k-NN 是一種<strong>監督式學習 (Supervised Learning)</strong> 演算法，既可以用於<strong>分類 (Classification)</strong>，也可以用於<strong>回歸 (Regression)</strong>。</p>
<p>它的工作原理非常直觀：當我們要預測一個新數據點的類別時，演算法會在訓練資料集中尋找與該點「距離最近」的 <strong>k</strong> 個鄰居。接著，根據這 k 個鄰居的類別進行「投票」，票數最多的類別就是該新數據點的預測結果。</p>
<p><img src="https://hackmd.io/_uploads/BJPCNfnubg.png" alt="kNN示意圖"><br><a target="_blank" rel="noopener" href="https://ai.plainenglish.io/introduction-to-k-nearest-neighbors-knn-algorithm-e8617a448fa8?gi=2778ea2c9b46">來源</a></p>
<hr>
<h2 id="2-k-NN-的運作步驟"><a href="#2-k-NN-的運作步驟" class="headerlink" title="2. k-NN 的運作步驟"></a>2. k-NN 的運作步驟</h2><ol>
<li><strong>選擇 k 值</strong>：決定要參考多少個鄰居（例如 k=3 或 k=5）。</li>
<li><strong>計算距離</strong>：計算新數據點與訓練集中所有點的距離。</li>
<li><strong>尋找鄰居</strong>：找出距離最近的 k 個點。</li>
<li><strong>進行預測</strong>：<ul>
<li><strong>分類問題</strong>：採用「多數決」，將新點歸類為鄰居中出現次數最多的類別。</li>
<li><strong>回歸問題</strong>：計算 k 個鄰居數值的「平均值」作為預測結果。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="3-如何計算「距離」？"><a href="#3-如何計算「距離」？" class="headerlink" title="3. 如何計算「距離」？"></a>3. 如何計算「距離」？</h2><p>「距離」的定義決定了鄰居的選取。最常用的計算方法包括：</p>
<ul>
<li><strong>歐氏距離 (Euclidean Distance)</strong>：最常見的直線距離（L2 範數）。</li>
</ul>
<p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -3.453ex;" xmlns="http://www.w3.org/2000/svg" width="19.245ex" height="8.281ex" role="img" focusable="false" viewBox="0 -2133.6 8506.5 3660"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(797.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msqrt" transform="translate(1853.6,0)"><g transform="translate(1056,0)"><g data-mml-node="munderover"><g data-mml-node="mo"><path data-c="2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"></path></g><g data-mml-node="TeXAtom" transform="translate(148.2,-1087.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(509.9,1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1444,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(1833,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(2954.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msub" transform="translate(3954.4,0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="msup" transform="translate(4771.3,0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mn" transform="translate(422,289) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g><g data-mml-node="mo" transform="translate(0,-476.4)"><path data-c="E001" d="M702 589Q706 601 718 605H1061Q1076 597 1076 585Q1076 572 1061 565H742V0Q734 -14 724 -14H722H720Q708 -14 702 0V589Z" transform="translate(0,1945)"></path><path data-c="23B7" d="M742 -871Q740 -873 737 -876T733 -880T730 -882T724 -884T714 -885H702L222 569L180 484Q138 399 137 399Q131 404 124 412L111 425L265 736L702 -586V168L703 922Q713 935 722 935Q734 935 742 920V-871Z" transform="translate(0,-165)"></path><svg width="1056" height="1361" y="670" x="0" viewBox="0 295.5 1056 1361"><path data-c="E000" d="M722 -14H720Q708 -14 702 0V306L703 612Q713 625 722 625Q734 625 742 610V0Q734 -14 724 -14H722Z" transform="scale(1,3.195)"></path></svg></g><rect width="5596.9" height="60" x="1056" y="2013.6"></rect></g></g></g></svg></mjx-container>    </p>
<ul>
<li><strong>曼哈頓距離 (Manhattan Distance)</strong>：城市街區距離（L1 範數）。</li>
<li><strong>閔可夫斯基距離 (Minkowski Distance)</strong>：上述兩者的推廣形式。</li>
</ul>
<blockquote>
<p><strong>注意</strong>：由於距離計算對數值大小敏感，在使用 k-NN 前，務必對數據進行 <strong>特徵縮放 (Feature Scaling)</strong>（如標準化或歸一化），否則數值較大的特徵會主導距離運算。</p>
</blockquote>
<hr>
<h2 id="4-如何選擇關鍵的-k-值？"><a href="#4-如何選擇關鍵的-k-值？" class="headerlink" title="4. 如何選擇關鍵的 k 值？"></a>4. 如何選擇關鍵的 k 值？</h2><p>k 值的選擇會大幅影響模型的表現：</p>
<ul>
<li><strong>k 太小（如 k=1）</strong>：模型會對噪聲非常敏感，容易產生<strong>過擬合 (Overfitting)</strong>。</li>
<li><strong>k 太大（如 k=100）</strong>：邊界會變得模糊，容易產生<strong>欠擬合 (Underfitting)</strong>。</li>
</ul>
<p><strong>最佳實作建議</strong>：通常使用<strong>交叉驗證 (Cross-validation)</strong> 來尋找最適合該資料集的 k 值。此外，為了避免平手（票數相同），k 通常建議選擇<strong>奇數</strong>。</p>
<hr>
<h2 id="5-k-NN-的優缺點分析"><a href="#5-k-NN-的優缺點分析" class="headerlink" title="5. k-NN 的優缺點分析"></a>5. k-NN 的優缺點分析</h2><h3 id="優點"><a href="#優點" class="headerlink" title="優點"></a>優點</h3><ul>
<li><strong>簡單易懂</strong>：不需建立複雜模型，易於實現。</li>
<li><strong>無假設</strong>：屬於非參數化模型，不假設數據分佈。</li>
<li><strong>適合多分類</strong>：處理多種類別的分類問題非常直觀。</li>
</ul>
<h3 id="缺點"><a href="#缺點" class="headerlink" title="缺點"></a>缺點</h3><ul>
<li><strong>計算代價高</strong>：每次預測都要掃描整個資料集計算距離，當數據量大時速度極慢。</li>
<li><strong>記憶體消耗大</strong>：需要儲存所有的訓練數據。</li>
<li><strong>維度災難</strong>：在高維度空間中，點與點之間的距離會變得很近，導致分類效果變差。</li>
</ul>
<p>k-NN 雖然在處理大規模數據時效率不高，但其「懶惰學習 (Lazy Learning)」的特性（不需要事先訓練模型），使其在小型數據集或異常檢測任務中仍非常實用。</p>
<hr>
<h3>用 Python 從零實作 k-NN 演算法與進階加權技巧</h3>

<p>在學習機器學習時，直接套用 <code>scikit-learn</code> 固然快速，但若能親手用原生 Python 刻出演算法的底層邏輯，對觀念的理解將會大躍進。今天我們將以<strong>糖尿病預測資料集 (Diabetes Dataset)</strong> 為例，不依賴機器學習套件，從零開始實作 <strong>k-NN (k-Nearest Neighbors) 演算法</strong>，並進階探討「曼哈頓距離」與「距離加權」的優化技巧。</p>
<p>資料集來源：<a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/johndasilva/diabetes">https://www.kaggle.com/datasets/johndasilva/diabetes</a></p>
<p>訓練集樣本數：567</p>
<p>Code:<br><a target="_blank" rel="noopener" href="https://github.com/ichika2004/kNN/blob/main/Data_mining.ipynb">https://github.com/ichika2004/kNN/blob/main/Data_mining.ipynb</a></p>
<hr>
<h2 id="1-資料預處理：為什麼特徵縮放如此重要？"><a href="#1-資料預處理：為什麼特徵縮放如此重要？" class="headerlink" title="1. 資料預處理：為什麼特徵縮放如此重要？"></a>1. 資料預處理：為什麼特徵縮放如此重要？</h2><p>k-NN 是一個高度依賴「距離」的演算法。如果資料集中某個特徵的數值極大（例如胰島素濃度），而另一個特徵的數值極小（例如懷孕次數），在計算距離時，大數值的特徵就會完全主導結果。</p>
<p>因此，在進入模型之前，我們必須自己寫一個標準化（Z-score Standardization）的函數，將所有特徵縮放到相同的尺度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 標準化函數：(數值 - 平均值) / 標準差</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">standardize</span>(<span class="params">df</span>):</span><br><span class="line">    df = (df - df.mean()) / df.std()</span><br><span class="line">    <span class="keyword">return</span> df</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="2-核心實作：基礎-k-NN-歐氏距離與多數決"><a href="#2-核心實作：基礎-k-NN-歐氏距離與多數決" class="headerlink" title="2. 核心實作：基礎 k-NN (歐氏距離與多數決)"></a>2. 核心實作：基礎 k-NN (歐氏距離與多數決)</h2><p>定義 <code>knn</code> 函數：針對測試集中的每一筆資料，我們需要：</p>
<ol>
<li>計算它與訓練集中所有資料的<strong>歐氏距離 (Euclidean Distance)</strong>。</li>
<li>將距離由小到大排序。</li>
<li>取出距離最近的 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.011ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 889 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g></g></g></svg></mjx-container> 個鄰居。</li>
<li>進行「多數決」投票，票數高者即為預測結果。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">knn</span>(<span class="params">train_data, test_data, k</span>):</span><br><span class="line">    predictions = []</span><br><span class="line">    <span class="keyword">for</span> index, test_instance <span class="keyword">in</span> test_data.iterrows():</span><br><span class="line">        distances = []</span><br><span class="line">        <span class="comment"># 1. 計算與所有訓練資料的距離</span></span><br><span class="line">        <span class="keyword">for</span> _, train_instance <span class="keyword">in</span> train_data.iterrows():</span><br><span class="line">            dist = mt.sqrt((test_instance[<span class="number">0</span>] - train_instance[<span class="number">0</span>])**<span class="number">2</span> +</span><br><span class="line">                           (test_instance[<span class="number">1</span>] - train_instance[<span class="number">1</span>])**<span class="number">2</span> +</span><br><span class="line">                           (test_instance[<span class="number">2</span>] - train_instance[<span class="number">2</span>])**<span class="number">2</span> +</span><br><span class="line">                           (test_instance[<span class="number">3</span>] - train_instance[<span class="number">3</span>])**<span class="number">2</span> +</span><br><span class="line">                           (test_instance[<span class="number">4</span>] - train_instance[<span class="number">4</span>])**<span class="number">2</span> +</span><br><span class="line">                           (test_instance[<span class="number">5</span>] - train_instance[<span class="number">5</span>])**<span class="number">2</span> +</span><br><span class="line">                           (test_instance[<span class="number">6</span>] - train_instance[<span class="number">6</span>])**<span class="number">2</span> +</span><br><span class="line">                           (test_instance[<span class="number">7</span>] - train_instance[<span class="number">7</span>])**<span class="number">2</span>)</span><br><span class="line">            distances.append((dist, train_instance[<span class="string">'Outcome'</span>]))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. 距離由小到大排序</span></span><br><span class="line">        distances.sort(key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>])</span><br><span class="line">        <span class="comment"># 3. 尋找最近的 k 個鄰居</span></span><br><span class="line">        neighbors = distances[:k] </span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4. 多數決投票</span></span><br><span class="line">        votes = {<span class="string">'0'</span>: <span class="number">0</span>, <span class="string">'1'</span>: <span class="number">0</span>}</span><br><span class="line">        <span class="keyword">for</span> _, outcome <span class="keyword">in</span> neighbors:</span><br><span class="line">            votes[<span class="built_in">str</span>(<span class="built_in">int</span>(outcome))] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        predicted_class = <span class="built_in">max</span>(votes, key=votes.get)</span><br><span class="line">        predictions.append(<span class="built_in">int</span>(predicted_class))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> predictions</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="3-尋找最佳的-K-值"><a href="#3-尋找最佳的-K-值" class="headerlink" title="3. 尋找最佳的 K 值"></a>3. 尋找最佳的 K 值</h2><p>K 值的選擇會大幅影響準確率。一般來說，K 值建議選擇<strong>奇數</strong>（避免平手），且最大搜尋範圍通常設定在<strong>訓練集資料筆數的平方根</strong>。我們可以使用迴圈來測試不同的 K 值，並視覺化結果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 取得訓練資料筆數的平方根做為最大 K 值</span></span><br><span class="line">k_max = <span class="built_in">int</span>(mt.sqrt(<span class="built_in">len</span>(train_data)))  </span><br><span class="line">k_values = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">1</span>, k_max, <span class="number">2</span>))</span><br><span class="line">accuracy_euil = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 測試不同的 K 值</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> k_values:</span><br><span class="line">    predictions = knn(train_data, test_data, k)</span><br><span class="line">    true_outcome = test_data[<span class="string">'Outcome'</span>].tolist()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 計算準確率</span></span><br><span class="line">    correct_predictions = <span class="built_in">sum</span>(<span class="number">1</span> <span class="keyword">for</span> true, pred <span class="keyword">in</span> <span class="built_in">zip</span>(true_outcome, predictions) <span class="keyword">if</span> true == pred)</span><br><span class="line">    accuracy = correct_predictions / <span class="built_in">len</span>(true_outcome)</span><br><span class="line">    accuracy_euil.append(accuracy)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id=""><a href="#" class="headerlink" title=""></a><img src="https://hackmd.io/_uploads/B10tWbn_Ze.png" alt="不同k值的準確率"></h2><h2 id="4-進階實作-一-：變更距離標準-曼哈頓距離"><a href="#4-進階實作-一-：變更距離標準-曼哈頓距離" class="headerlink" title="4. 進階實作 (一)：變更距離標準 - 曼哈頓距離"></a>4. 進階實作 (一)：變更距離標準 - 曼哈頓距離</h2><p>除了直線的歐氏距離，有時候在特定維度特徵下，使用<strong>曼哈頓距離 (Manhattan Distance, 絕對值相加)</strong> 會有更好的效果。實作上，我們只需要修改距離的計算方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 曼哈頓距離計算範例</span></span><br><span class="line">dist = (<span class="built_in">abs</span>(test_instance[<span class="number">0</span>] - train_instance[<span class="number">0</span>]) +</span><br><span class="line">        <span class="built_in">abs</span>(test_instance[<span class="number">1</span>] - train_instance[<span class="number">1</span>]) +</span><br><span class="line">        <span class="built_in">abs</span>(test_instance[<span class="number">2</span>] - train_instance[<span class="number">2</span>]) +</span><br><span class="line">        <span class="comment"># ...省略其餘特徵...</span></span><br><span class="line">        <span class="built_in">abs</span>(test_instance[<span class="number">7</span>] - train_instance[<span class="number">7</span>]))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="https://hackmd.io/_uploads/SJxfm-hOZx.png" alt="將歐式距離改成曼哈頓距離"></p>
<hr>
<h2 id="5-進階實作-二-：距離加權投票-Weighted-k-NN"><a href="#5-進階實作-二-：距離加權投票-Weighted-k-NN" class="headerlink" title="5. 進階實作 (二)：距離加權投票 (Weighted k-NN)"></a>5. 進階實作 (二)：距離加權投票 (Weighted k-NN)</h2><p>傳統 k-NN 有個缺點：「只要擠進前 K 名，每一票的份量都一樣重」。但直覺上，<strong>距離越近的鄰居，應該要有越高的參考價值</strong>。這時我們可以引入「加權機制」。</p>
<h3 id="方法-1：反函數加權-Inverse-Distance-Weighting"><a href="#方法-1：反函數加權-Inverse-Distance-Weighting" class="headerlink" title="方法 1：反函數加權 (Inverse Distance Weighting)"></a>方法 1：反函數加權 (Inverse Distance Weighting)</h3><p>將距離取倒數，距離越近，權重越大。（加上 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="5.295ex" height="2.005ex" role="img" focusable="false" viewBox="0 -864 2340.2 886"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"></path></g><g data-mml-node="TeXAtom" transform="translate(1033,393.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(778,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"></path></g></g></g></g></g></svg></mjx-container> 是為了避免距離為 0 時發生除以零的錯誤）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">votes = {<span class="string">'0'</span>: <span class="number">0</span>, <span class="string">'1'</span>: <span class="number">0</span>}</span><br><span class="line"><span class="keyword">for</span> dist, outcome <span class="keyword">in</span> neighbors:</span><br><span class="line">    votes[<span class="built_in">str</span>(<span class="built_in">int</span>(outcome))] += <span class="number">1</span> / (dist + <span class="number">10</span>**(-<span class="number">10</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="https://hackmd.io/_uploads/BJmv7ZnuWx.png" alt="使用反函數加權"></p>
<h3 id="方法-2：高斯函數加權-Gaussian-Weighting"><a href="#方法-2：高斯函數加權-Gaussian-Weighting" class="headerlink" title="方法 2：高斯函數加權 (Gaussian Weighting)"></a>方法 2：高斯函數加權 (Gaussian Weighting)</h3><p>高斯函數（常態分佈曲線）是一種更平滑的加權方式。給定一個參數 <code>sigma</code> 來控制權重遞減的速度。</p>
<p><img src="https://hackmd.io/_uploads/B1iiX-2OZe.png" alt="不同sigma下的高斯函數"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sigma 為自訂參數</span></span><br><span class="line"><span class="keyword">for</span> _, train_instance <span class="keyword">in</span> train_data.iterrows():</span><br><span class="line">    <span class="comment"># ...前面計算歐氏距離 (dist)...</span></span><br><span class="line">    weight = mt.exp(-dist**<span class="number">2</span> / (<span class="number">2</span> * (sigma**<span class="number">2</span>)))</span><br><span class="line">    distances.append((weight, train_instance[<span class="string">'Outcome'</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 依照權重由大到小排序 (因為權重越大代表越近)</span></span><br><span class="line">distances.sort(key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">neighbors = distances[:k]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> weight, outcome <span class="keyword">in</span> neighbors:</span><br><span class="line">    votes[<span class="built_in">str</span>(<span class="built_in">int</span>(outcome))] += weight</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="https://hackmd.io/_uploads/B1IR7b2Obg.png" alt="使用高斯函數加權"></p>
<p><strong>討論：為何反函數加權結果與一般K-NN相似</strong></p>
<p>這點可以從高斯函數解釋，當執行標準化後，資料尺度被縮到(-1,1)，導致其權重相差不大</p>
<hr>
<h2 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h2><p>透過親手實作 <code>knn</code> 函數，我們不僅徹底理解了資料標準化的重要性，也掌握了如何抽換「距離公式」與「投票加權機制」。這些底層邏輯的熟悉度，將成為未來在微調機器學習模型參數時，最強大的基礎！</p>
<p><strong>標籤：</strong> #Python #機器學習 #DataMining #kNN演算法 #演算法實作 #資料科學</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://ichika2004.github.io/Blog/2023/05/19/Diabetes-prediction-using-K-Nearest-Neighbors/" data-id="cuidjXgVO7GRiYZ2CQw_LNyL9" data-title="機器學習入門：k-NN (k-Nearest Neighbors) 演算法詳解" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/Blog/2026/02/24/%E7%AC%AC%E4%B8%80%E9%80%B1/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Week 1
        
      </div>
    </a>
  
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/Blog/categories/machine-learning/">機器學習筆記</a></li><li class="category-list-item"><a class="category-list-link" href="/Blog/categories/bioinformatics/">生物資訊初探</a></li><li class="category-list-item"><a class="category-list-link" href="/Blog/categories/%E9%96%92%E8%81%8A%E8%88%87%E5%85%AC%E5%91%8A/">閒聊與公告</a></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/Blog/archives/2026/02/">February 2026</a></li><li class="archive-list-item"><a class="archive-list-link" href="/Blog/archives/2023/05/">May 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/Blog/2026/02/25/%E6%AD%A1%E8%BF%8E%E4%BE%86%E5%88%B0-Joeys-Blog/">沒什麼，只是 Joey的 Blog</a>
          </li>
        
          <li>
            <a href="/Blog/2026/02/24/%E7%AC%AC%E4%B8%80%E9%80%B1/">Week 1</a>
          </li>
        
          <li>
            <a href="/Blog/2023/05/19/Diabetes-prediction-using-K-Nearest-Neighbors/">機器學習入門：k-NN (k-Nearest Neighbors) 演算法詳解</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2026 Joey<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/Blog/" class="mobile-nav-link">Home</a>
  
    <a href="/Blog/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/Blog/categories/bioinformatics/" class="mobile-nav-link">生物資訊初探</a>
  
    <a href="/Blog/categories/machine-learning/" class="mobile-nav-link">機器學習筆記</a>
  
</nav>
    


<script src="/Blog/js/jquery-3.6.4.min.js"></script>



  
<script src="/Blog/fancybox/jquery.fancybox.min.js"></script>




<script src="/Blog/js/script.js"></script>





  </div>
</body>
</html>